{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install timm torch torchvision"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hk63coBKQ4VG",
    "outputId": "5fbdda1b-95a8-4c0b-b88a-8b55f648e626"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.22)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cpu)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm) (6.0.3)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm) (0.36.0)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm) (0.7.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (25.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (1.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2025.11.12)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import timm\n",
    "\n",
    "class ConvLoRA(nn.Module):\n",
    "    \"\"\"\n",
    "    LoRA implemented for Conv2d layers.\n",
    "    It wraps a specific target Conv2d layer, freezes it, and adds the A-B paths.\n",
    "    \"\"\"\n",
    "    def __init__(self, target_conv: nn.Conv2d, r: int = 8, alpha: int = 16):\n",
    "        super().__init__()\n",
    "        self.target_conv = target_conv\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / r\n",
    "\n",
    "        # Freeze the original target layer\n",
    "        self.target_conv.weight.requires_grad = False\n",
    "        if self.target_conv.bias is not None:\n",
    "            self.target_conv.bias.requires_grad = False\n",
    "\n",
    "        # Extract attributes from target\n",
    "        in_channels = target_conv.in_channels\n",
    "        out_channels = target_conv.out_channels\n",
    "        kernel_size = target_conv.kernel_size\n",
    "        stride = target_conv.stride\n",
    "        padding = target_conv.padding\n",
    "        groups = target_conv.groups\n",
    "\n",
    "        # --- LoRA Layers ---\n",
    "        # Matrix A: Down-projection.\n",
    "        # We match kernel size/stride/padding to maintain spatial consistency.\n",
    "        self.lora_A = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=r,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            bias=False\n",
    "        )\n",
    "\n",
    "        # Matrix B: Up-projection.\n",
    "        # Using 1x1 to act as a linear mixing layer back to dimension d.\n",
    "        self.lora_B = nn.Conv2d(\n",
    "            in_channels=r,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=False\n",
    "        )\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Initialize A with Kaiming, B with Zeros (so LoRA starts as identity)\n",
    "        nn.init.kaiming_uniform_(self.lora_A.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Original frozen path\n",
    "        original_out = self.target_conv(x)\n",
    "\n",
    "        # LoRA path\n",
    "        lora_out = self.lora_B(self.lora_A(x)) * self.scaling\n",
    "\n",
    "        return original_out + lora_out"
   ],
   "metadata": {
    "id": "I7sCUNw6RYV3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def apply_lora_to_fastvit(model, r=8, alpha=16, target_blocks=[0, 1, 2, 3]):\n",
    "    \"\"\"\n",
    "    Iterates through the FastViT stages and injects LoRA into Conv2d layers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Helper to recursively swap layers\n",
    "    def replace_conv_with_lora(module):\n",
    "        for name, child in module.named_children():\n",
    "            if isinstance(child, nn.Conv2d):\n",
    "                # Heuristic: LoRA the 1x1 projection layers (pointwise)\n",
    "                # For FastViT efficiency, we target layers with kernel_size=1 or larger filters.\n",
    "                if child.kernel_size == (1, 1) or child.kernel_size == 1:\n",
    "                    print(f\"  -> Patching Layer: {name} | Shape: {child.weight.shape}\")\n",
    "                    setattr(module, name, ConvLoRA(child, r=r, alpha=alpha))\n",
    "            else:\n",
    "                # Recursively search deeper\n",
    "                replace_conv_with_lora(child)\n",
    "\n",
    "    print(f\"Integrate LoRA (r={r}) into FastViT...\")\n",
    "\n",
    "    stages = model.stages # Access the main 4 stages\n",
    "\n",
    "    for i, stage in enumerate(stages):\n",
    "        if i in target_blocks:\n",
    "            print(f\"\\nProcessing Stage {i}...\")\n",
    "            replace_conv_with_lora(stage)\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "id": "fVQ9kvt6RcBk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 1. Load Pre-trained FastViT\n",
    "# We use 'fastvit_t8' as a lightweight example.\n",
    "model = timm.create_model('fastvit_t8', pretrained=True)\n",
    "\n",
    "# 2. Check Parameters before LoRA\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Original Params: {total_params:,} | Trainable: {trainable_params:,}\")\n",
    "\n",
    "# 3. Apply LoRA\n",
    "# We patch all 4 blocks (0, 1, 2, 3)\n",
    "model = apply_lora_to_fastvit(model, r=16, alpha=16)\n",
    "\n",
    "# 4. Check Parameters after LoRA\n",
    "total_params_lora = sum(p.numel() for p in model.parameters())\n",
    "trainable_params_lora = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(f\"LoRA Adapted Model Stats:\")\n",
    "print(f\"Total Params: {total_params_lora:,}\")\n",
    "print(f\"Trainable Params: {trainable_params_lora:,}\")\n",
    "print(f\"Percentage Trainable: {(trainable_params_lora/total_params_lora)*100:.2f}%\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# 5. Dummy Forward Pass to ensure shapes are correct\n",
    "dummy_input = torch.randn(1, 3, 256, 256)\n",
    "output = model(dummy_input)\n",
    "print(f\"\\nForward pass successful. Output shape: {output.shape}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 978,
     "referenced_widgets": [
      "6463239574094fcfa18ca60e95851db7",
      "e68d555bbca240adb8d9f434ce65a988",
      "0ac66c39b2774bbdb6670874815eada8",
      "49538b7d481d46ee87d2cabb3f471592",
      "75b1d64a476f4e4482e1984eb6837899",
      "c7e660e81a1c43bfb02cd5dd97dc2fd5",
      "8bdf7944d563452b9a2e49e50d99efce",
      "a4eea6438795420c9746852db18118e5",
      "c8560baf5ac240d7b81125604df9fb31",
      "a1dde7ff1ac240b08c1bc15bb3aed2a8",
      "2fc0948d85f64645b822198db7559421"
     ]
    },
    "id": "TveBejlGRd0d",
    "outputId": "ae2c4baa-366c-4a6b-f386-f25e6476342e"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/16.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original Params: 4,026,232 | Trainable: 4,026,232\n",
      "Integrate LoRA (r=16) into FastViT...\n",
      "\n",
      "Processing Stage 0...\n",
      "  -> Patching Layer: conv | Shape: torch.Size([48, 1, 1, 1])\n",
      "  -> Patching Layer: fc1 | Shape: torch.Size([144, 48, 1, 1])\n",
      "  -> Patching Layer: fc2 | Shape: torch.Size([48, 144, 1, 1])\n",
      "  -> Patching Layer: conv | Shape: torch.Size([48, 1, 1, 1])\n",
      "  -> Patching Layer: fc1 | Shape: torch.Size([144, 48, 1, 1])\n",
      "  -> Patching Layer: fc2 | Shape: torch.Size([48, 144, 1, 1])\n",
      "\n",
      "Processing Stage 1...\n",
      "  -> Patching Layer: conv | Shape: torch.Size([96, 96, 1, 1])\n",
      "  -> Patching Layer: conv | Shape: torch.Size([96, 1, 1, 1])\n",
      "  -> Patching Layer: fc1 | Shape: torch.Size([288, 96, 1, 1])\n",
      "  -> Patching Layer: fc2 | Shape: torch.Size([96, 288, 1, 1])\n",
      "  -> Patching Layer: conv | Shape: torch.Size([96, 1, 1, 1])\n",
      "  -> Patching Layer: fc1 | Shape: torch.Size([288, 96, 1, 1])\n",
      "  -> Patching Layer: fc2 | Shape: torch.Size([96, 288, 1, 1])\n",
      "\n",
      "Processing Stage 2...\n",
      "  -> Patching Layer: conv | Shape: torch.Size([192, 192, 1, 1])\n",
      "  -> Patching Layer: conv | Shape: torch.Size([192, 1, 1, 1])\n",
      "  -> Patching Layer: fc1 | Shape: torch.Size([576, 192, 1, 1])\n",
      "  -> Patching Layer: fc2 | Shape: torch.Size([192, 576, 1, 1])\n",
      "  -> Patching Layer: conv | Shape: torch.Size([192, 1, 1, 1])\n",
      "  -> Patching Layer: fc1 | Shape: torch.Size([576, 192, 1, 1])\n",
      "  -> Patching Layer: fc2 | Shape: torch.Size([192, 576, 1, 1])\n",
      "  -> Patching Layer: conv | Shape: torch.Size([192, 1, 1, 1])\n",
      "  -> Patching Layer: fc1 | Shape: torch.Size([576, 192, 1, 1])\n",
      "  -> Patching Layer: fc2 | Shape: torch.Size([192, 576, 1, 1])\n",
      "  -> Patching Layer: conv | Shape: torch.Size([192, 1, 1, 1])\n",
      "  -> Patching Layer: fc1 | Shape: torch.Size([576, 192, 1, 1])\n",
      "  -> Patching Layer: fc2 | Shape: torch.Size([192, 576, 1, 1])\n",
      "\n",
      "Processing Stage 3...\n",
      "  -> Patching Layer: conv | Shape: torch.Size([384, 384, 1, 1])\n",
      "  -> Patching Layer: conv | Shape: torch.Size([384, 1, 1, 1])\n",
      "  -> Patching Layer: fc1 | Shape: torch.Size([1152, 384, 1, 1])\n",
      "  -> Patching Layer: fc2 | Shape: torch.Size([384, 1152, 1, 1])\n",
      "  -> Patching Layer: conv | Shape: torch.Size([384, 1, 1, 1])\n",
      "  -> Patching Layer: fc1 | Shape: torch.Size([1152, 384, 1, 1])\n",
      "  -> Patching Layer: fc2 | Shape: torch.Size([384, 1152, 1, 1])\n",
      "\n",
      "==============================\n",
      "LoRA Adapted Model Stats:\n",
      "Total Params: 4,339,576\n",
      "Trainable Params: 1,344,472\n",
      "Percentage Trainable: 30.98%\n",
      "==============================\n",
      "\n",
      "Forward pass successful. Output shape: torch.Size([1, 1000])\n"
     ]
    }
   ]
  }
 ]
}